{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0043b45f",
   "metadata": {},
   "source": [
    "# **Titanic Survival Prediction**\n",
    "\n",
    "**Author:** Milos Saric [https://saricmilos.com/]  \n",
    "**YOUTUBE: English: @realskillsoverdegrees  Serbian: @saricmilos**  \n",
    "**Date:** October 7, 2025  \n",
    "**Dataset:** Titanic Passenger Data  \n",
    "\n",
    "---\n",
    "\n",
    "This notebook explores the classic Titanic dataset to predict passenger survival using machine learning.  \n",
    "The analysis will guide you through the full data science workflow, including:\n",
    "\n",
    "1. **Problem Definition** – Clearly outline the objective and scope of the project.\n",
    "\n",
    "2. **Data Collection** – Gather relevant datasets from KAGGLE.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)** – Analyze and visualize data to uncover patterns and insights.\n",
    "\n",
    "4. **Feature Engineering** – Create, transform, or select meaningful features to improve model performance.\n",
    "\n",
    "5. **Model Development** – Build and train predictive or analytical models.\n",
    "\n",
    "6. **Evaluation & Testing** – Assess model performance using appropriate metrics and validate results.\n",
    "\n",
    "The goal of this project is to apply practical data science techniques to a real-world dataset and gain insights into the factors that influenced survival on the Titanic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a6e89",
   "metadata": {},
   "source": [
    "## 1. **Problem Definition**\n",
    "\n",
    "This phase involves clearly understanding the challenge we aim to solve. This step sets the foundation for the entire project and ensures all efforts are aligned toward a common goal.\n",
    "\n",
    "Key aspects include:\n",
    "\n",
    "- **Objective**: Predict whether a passenger survived the Titanic disaster based on available features such as age, gender, class, fare and newly created features.  \n",
    "\n",
    "- **Scope**: The analysis focuses on the provided Titanic dataset. Predictions are limited to the passengers listed in the dataset, without considering external historical data or additional features beyond what is provided.  \n",
    "\n",
    "- **Stakeholders**:  \n",
    "  - **Data Scientists / ML Practitioners**: To practice and improve predictive modeling skills.  \n",
    "  - **Kaggle Community**: Participants competing in the Titanic challenge.  \n",
    "  - **Educators / Students**: Learning tool for understanding classification problems and feature engineering.  \n",
    "\n",
    "- **Success Criteria**: Achieve high prediction accuracy on the test dataset, evaluated using metrics such as **accuracy score**. A successful model reliably distinguishes between survivors and non-survivors.\n",
    "\n",
    ">A well-defined problem statement is half the solution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39876ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=r\"C:\\Users\\Milos\\Desktop\\ESCAPE 9-5\\PYTHON\\GitHub Kaggle Projects\\1. Titanic Survival Predictor\\Images\\titanic.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a4b19",
   "metadata": {},
   "source": [
    "## **2. Data Collection**\n",
    "\n",
    "The **Data Collection** phase is all about gathering the data we need and setting up the tools for analysis. In this step, we also import essential libraries and create reusable functions to streamline our workflow.\n",
    "The training and testing datasets for this project are provided by Kaggle. You can either:\n",
    "\n",
    " - **1.** Download them directly from my GitHub: https://github.com/saricmilos/titanic-survival-prediction\n",
    "\n",
    " - **2.** Or access them from Kaggle itself: Titanic: Machine Learning from Disaster\n",
    "\n",
    "Both sources contain the same dataset, so you can choose whichever is more convenient.\n",
    "\n",
    "## **2.1. Import Libraries**\n",
    "   Import libraries for data handling, visualization, and modeling:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81808d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "#HyperParameters\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5069489",
   "metadata": {},
   "source": [
    "## **2.2. Create Reusable Functions**\n",
    "Functions to avoid repetitive tasks and keep code clean:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76258237",
   "metadata": {},
   "source": [
    "### **2.2.1. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our datasets\n",
    "def load_dataset(csv_path: Path, **read_csv_kwargs: Any) -> pd.DataFrame:\n",
    "    \"\"\"     \n",
    "    Load a CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (Path): Full path to the CSV file\n",
    "        **read_csv_kwargs: Optional arguments for pd.read_csv\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "     \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "    return pd.read_csv(csv_path, **read_csv_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fedcb",
   "metadata": {},
   "source": [
    "### **2.2.1. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title from the name\n",
    "def extract_title(name):\n",
    "    match = re.search(r\", (\\w+)\\.\",name)\n",
    "    return match.group(1) if match else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column containing each passenger title as categorical numerical value\n",
    "def process_titles(df, rare_titles, title_mapping):\n",
    "    \"\"\"\n",
    "    Extracts and encodes passenger titles into numeric categories.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the dataset to process\n",
    "    - rare_titles: list of titles to group as 'Rare'\n",
    "    - title_mapping: dict mapping titles to numeric values\n",
    "    \n",
    "    Returns:\n",
    "    - df: DataFrame with a new 'Title' column encoded numerically\n",
    "    \"\"\"\n",
    "    # Extract titles\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    \n",
    "    # Replace rare titles\n",
    "    df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n",
    "    \n",
    "    # Ensure all remaining titles exist in the mapping\n",
    "    df['Title'] = df['Title'].apply(lambda x: x if x in title_mapping else 'Unknown')\n",
    "    \n",
    "    # Map to numeric\n",
    "    df['Title'] = df['Title'].map(title_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a68fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine size of the family from number of family members (cousins, children, parents):\n",
    "def family_category(size):\n",
    "    if size == 1:\n",
    "        return \"Single\"\n",
    "    elif size <= 4:\n",
    "        return \"SmallFamily\"\n",
    "    else:\n",
    "        return \"LargeFamily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a column for family oriented features (size of the family, travelling alone, number of members in each person family)\n",
    "def process_family_features(df, family_category_func, family_mapping):\n",
    "    \"\"\"\n",
    "    Adds family-related features to a DataFrame:\n",
    "    - FamilySize: total number of family members aboard\n",
    "    - FamilyCategory: categorical encoding of family size\n",
    "    - IsAlone: 1 if the passenger is alone, 0 otherwise\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - family_category_func: function to categorize family size\n",
    "    - family_mapping: dict mapping family categories to numeric values\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with new family features\n",
    "    \"\"\"\n",
    "    # Compute family size\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    \n",
    "    # Categorize family size and map to numeric\n",
    "    df[\"FamilyCategory\"] = df[\"FamilySize\"].apply(family_category_func).map(family_mapping)\n",
    "    \n",
    "    # Flag passengers who are alone\n",
    "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f1fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_age_features(df, bins, labels, age_mapping):\n",
    "    \"\"\"\n",
    "    Adds age-related features to a DataFrame:\n",
    "    - AgeMissing: 1 if Age is missing, 0 otherwise\n",
    "    - Age: fills missing values using median per Title\n",
    "    - AgeGroup: numeric age group for modeling\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - bins: list of numeric bin edges for age groups\n",
    "    - labels: list of labels for each age group\n",
    "    - age_mapping: dict mapping age group labels to numeric codes\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with new age features\n",
    "    \"\"\"\n",
    "    # Flag missing ages\n",
    "    df[\"AgeMissing\"] = df[\"Age\"].isna().astype(int)\n",
    "    \n",
    "    # Fill missing ages with median per Title\n",
    "    df[\"Age\"] = df.groupby(\"Title\")[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # Categorize ages into bins\n",
    "    age_groups = pd.cut(df[\"Age\"], bins=bins, labels=labels)\n",
    "    \n",
    "    # Map labels to numeric codes and convert to integer\n",
    "    df[\"AgeGroup\"] = age_groups.map(age_mapping).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d21be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name=None, labels=None, figsize=(6, 4), normalize=False):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix using Seaborn.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true : array-like, true labels\n",
    "        y_pred : array-like, predicted labels\n",
    "        model_name : str, optional, name of the model for the title\n",
    "        labels : list, optional, class labels\n",
    "        figsize : tuple, optional, size of the figure\n",
    "        normalize : bool, optional, normalize counts to percentages\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\",\n",
    "                cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=labels if labels is not None else True,\n",
    "                yticklabels=labels if labels is not None else True)\n",
    "    \n",
    "    title = \"Confusion Matrix\"\n",
    "    if model_name:\n",
    "        title += f\" - {model_name}\"\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fare_bins(df, column=\"Fare\", bins=4, labels=None):\n",
    "    \"\"\"\n",
    "    Converts a continuous fare column into quantile-based bins, handling missing values,\n",
    "    and ensures the bin column is numeric.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to bin\n",
    "    - bins: number of quantile bins\n",
    "    - labels: list of labels for each bin (numeric or categorical)\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with new 'FareBin' column as numeric\n",
    "    \"\"\"\n",
    "    # Fill missing fares with median\n",
    "    df[column] = df[column].fillna(df[column].median())\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = list(range(bins))\n",
    "    \n",
    "    # Create quantile bins\n",
    "    fare_groups = pd.qcut(df[column], q=bins, labels=labels)\n",
    "    \n",
    "    # Convert to numeric\n",
    "    df[\"FareBin\"] = fare_groups.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ad089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df, column, encoding_type=\"onehot\", prefix=None, dummy_na=True):\n",
    "    \"\"\"\n",
    "    Encodes a categorical column in different ways and drops the original column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to encode\n",
    "    - encoding_type: str, type of encoding: \"onehot\" or \"label\"\n",
    "    - prefix: string to prefix dummy columns (only for one-hot encoding)\n",
    "    - dummy_na: bool, include a column for NaNs (only for one-hot encoding)\n",
    "    \n",
    "    Returns:\n",
    "    - df: DataFrame with encoded column(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    if encoding_type == \"onehot\":\n",
    "        if prefix is None:\n",
    "            prefix = column\n",
    "        # One-hot encode with optional NaN column\n",
    "        dummies = pd.get_dummies(df[column], prefix=prefix, dummy_na=dummy_na).astype(int)\n",
    "        df[dummies.columns] = dummies\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "    \n",
    "    elif encoding_type == \"label\":\n",
    "        # Label encode\n",
    "        le = LabelEncoder()\n",
    "        # Fill NaN temporarily to encode\n",
    "        df[column] = df[column].fillna(\"NaN\")  \n",
    "        df[column] = le.fit_transform(df[column])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported encoding_type. Choose 'onehot' or 'label'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65719574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_deck(df, all_decks):\n",
    "    \"\"\"\n",
    "    Extract deck from Cabin and one-hot encode it,\n",
    "    ensuring all columns exist and are in consistent order.\n",
    "    Drops the original Cabin column.\n",
    "    \"\"\"\n",
    "    # Extract deck, fill missing as \"Missing\"\n",
    "    df[\"Deck\"] = df[\"Cabin\"].apply(lambda x: str(x)[0] if pd.notna(x) else \"Missing\")\n",
    "    \n",
    "    # One-hot encode\n",
    "    deck_dummies = pd.get_dummies(df[\"Deck\"], prefix=\"Deck\").astype(int)\n",
    "    \n",
    "    # Add missing columns\n",
    "    for col in all_decks:\n",
    "        if col not in deck_dummies:\n",
    "            deck_dummies[col] = 0\n",
    "    \n",
    "    # Ensure column order\n",
    "    deck_dummies = deck_dummies[all_decks]\n",
    "    \n",
    "    # Add one-hot columns to DataFrame\n",
    "    df[all_decks] = deck_dummies\n",
    "    \n",
    "    # Drop original Cabin column\n",
    "    df.drop(columns=[\"Cabin\"], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240205cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticket(df, ticket_counts=None):\n",
    "    \"\"\"\n",
    "    Processes the Ticket column:\n",
    "    - Adds TicketGroupSize\n",
    "    - Optionally extracts TicketPrefix\n",
    "    - Drops original Ticket column\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - ticket_counts: precomputed ticket counts (dict or Series). \n",
    "                     If None, computes from df.\n",
    "\n",
    "    Returns:\n",
    "    - df: processed DataFrame\n",
    "    - ticket_counts: Series of ticket counts\n",
    "    \"\"\"\n",
    "    if ticket_counts is None:\n",
    "        ticket_counts = df['Ticket'].value_counts()\n",
    "    \n",
    "    # Ticket group size\n",
    "    df['TicketGroupSize'] = df['Ticket'].map(ticket_counts)\n",
    "    \n",
    "    # Ticket prefix\n",
    "    df['TicketPrefix'] = df['Ticket'].apply(lambda x: str(x).split()[0] if not str(x).isdigit() else 'None')\n",
    "    \n",
    "    # Drop original ticket\n",
    "    df.drop(columns=['Ticket'], inplace=True)\n",
    "    \n",
    "    return df, ticket_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f34f1",
   "metadata": {},
   "source": [
    "### **2.2.3. Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a34962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot most important features\n",
    "def plot_feature_importance(model, feature_names, top_n=15):\n",
    "    # Some models (like XGBoost/RandomForest) have 'feature_importances_'\n",
    "    importance = model.feature_importances_\n",
    "    fi = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(fi['Feature'], fi['Importance'])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Top {top_n} Feature Importances for {type(model).__name__}\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9553ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart(feature, dataset_name='train', datasets=None):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar chart of Survived vs Not Survived counts for a given feature.\n",
    "    Shows counts on bars and percentages in parentheses.\n",
    "    \"\"\"\n",
    "    if datasets is None or dataset_name not in datasets:\n",
    "        raise ValueError(\"Dataset not found in datasets dictionary\")\n",
    "    \n",
    "    df = datasets[dataset_name]\n",
    "\n",
    "    # Count values for each group\n",
    "    counts = df.groupby(['Survived', feature],observed=False).size().unstack(fill_value=0)\n",
    "\n",
    "    # Plot stacked bar\n",
    "    ax = counts.T.plot(kind='bar', stacked=True, figsize=(10,6), color=['red','green'])\n",
    "    \n",
    "    plt.title(f'Survival by {feature.capitalize()}')\n",
    "    plt.xlabel(feature.capitalize())\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Survived', labels=['Not Survived', 'Survived'])\n",
    "\n",
    "    # Add counts with percentages on bars\n",
    "    for i, col in enumerate(counts.columns):\n",
    "        total = counts[col].sum()  # total for this feature value\n",
    "        bottom = 0\n",
    "        for j in range(len(counts)):\n",
    "            height = counts.iloc[j, i]\n",
    "            if height > 0:\n",
    "                percent = height / total * 100\n",
    "                ax.text(\n",
    "                    i,  # x-coordinate = bar index\n",
    "                    bottom + height / 2,\n",
    "                    f'{int(height)} ({percent:.1f}%)',\n",
    "                    ha='center', va='center', color='white', fontsize=10\n",
    "                )\n",
    "            bottom += height\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84690b",
   "metadata": {},
   "source": [
    "## **2.3. Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3626cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path(r\"C:\\Users\\Milos\\Desktop\\ESCAPE 9-5\\PYTHON\\GitHub Kaggle Projects\\1. Titanic Survival Predictor\\Data\")\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9758bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in dataset_folder.glob(\"*.csv\"):\n",
    "    datasets[csv_file.stem] = load_dataset(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datasets.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15be174",
   "metadata": {},
   "source": [
    "##  **3. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "Exploratory Data Analysis is all about **understanding the dataset**, uncovering patterns, spotting anomalies, and generating insights that will guide feature engineering and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7049b9",
   "metadata": {},
   "source": [
    "## What Caused the “Unsinkable” Titanic to Go Down?\n",
    "\n",
    "1. **11:40 pm** – Titanic strikes an iceberg, seawater flooding her bow.  \n",
    "2. **12:00 am** – With the keel tilted upward, massive stress strains the hull.  \n",
    "3. **2:15 am** – The hull begins to break apart; Titanic splits along a joint.  \n",
    "4. **2:18 am** – The wheelhouse crumbles under the force of the sea.  \n",
    "5. **2:20 am** – The stern rises into the sky, floats for a brief, followed by Titanic sinking into the Atlantic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda72a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=r\"C:\\Users\\Milos\\Desktop\\ESCAPE 9-5\\PYTHON\\GitHub Kaggle Projects\\1. Titanic Survival Predictor\\Images\\howtitanicsank.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70839a52",
   "metadata": {},
   "source": [
    "Printing first 5 rows of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131561df",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdbb80",
   "metadata": {},
   "source": [
    "##  **Data Dictionary**\n",
    "\n",
    "| Feature       | Description |\n",
    "|---------------|-------------|\n",
    "| **PassengerId** | Unique identifier for each passenger |\n",
    "| **Survived**    | Survival status (0 = No, 1 = Yes) |\n",
    "| **Pclass**      | Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd) |\n",
    "| **Name**        | Full name of the passenger |\n",
    "| **Sex**         | Gender of the passenger (male/female) |\n",
    "| **Age**         | Age of the passenger in years |\n",
    "| **SibSp**       | Number of siblings or spouses aboard the Titanic |\n",
    "| **Parch**       | Number of parents or children aboard the Titanic |\n",
    "| **Ticket**      | Ticket number |\n",
    "| **Fare**        | Passenger fare (in British pounds) |\n",
    "| **Cabin**       | Cabin number |\n",
    "| **Embarked**    | Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5769d",
   "metadata": {},
   "source": [
    "There are 891 rows and 12 columns in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d69d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b666a",
   "metadata": {},
   "source": [
    "The test dataset contains 418 rows and 11 columns. Note that unlike the training dataset, it **does not include the target column `Survived`**, which we aim to predict using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc01f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76c782",
   "metadata": {},
   "source": [
    "We can observe that several features have missing values:\n",
    "\n",
    "- **Age**: Out of 891 rows in the training dataset, the Age is available for only 714 passengers, meaning 177 values are missing.  \n",
    "- **Cabin**: The Cabin feature is missing for the majority of passengers, with only 204 out of 891 rows containing a value.\n",
    "- **Embarked**: The Embarked feature is missing 2 values.    \n",
    "\n",
    "> Missing values are important to identify, as they may affect model performance and will need to be handled during data preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8aaae",
   "metadata": {},
   "source": [
    "It's important to inspect the **test dataset** for missing values before making predictions. The missing values for each column are as follows:\n",
    "\n",
    "- **Age**: Out of 418 rows in the test dataset, the Age is available for only **332 passengers**, meaning **86 values are missing**.  \n",
    "- **Cabin**: The Cabin feature is missing for most passengers, with only **91 out of 418 rows** containing a value.  \n",
    "- **Fare**: There is **1 missing value** in the Fare column.\n",
    "\n",
    "> Identifying missing values in the test dataset is important, as they need to be handled properly to ensure accurate predictions from our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65713e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][datasets[\"train\"][\"Survived\"] == 1][\"Sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e16a7",
   "metadata": {},
   "source": [
    "## **3.1. Bar Charts for Categorical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c398a",
   "metadata": {},
   "source": [
    "## Women and Children First\n",
    "The cry of *\"women and children first\"* echoed across the decks of the Titanic.  \n",
    "And it was obeyed.  \n",
    "\n",
    "While chaos spread through the freezing night, women and children were guided into lifeboats. Many men stepped back, allowing others a chance at survival. Some lived. Many did not.  \n",
    "\n",
    "The fate of each soul depended not only on courage, but also on where they stood on the ship when the iceberg struck.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95194f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_sex = bar_chart(\"Sex\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb07f",
   "metadata": {},
   "source": [
    "## Passenger Class and Survival\n",
    "\n",
    "The decks of the Titanic were divided not just by cabins, but by **class**—first, second, and third.  \n",
    "Where you slept often determined whether you lived or perished.  \n",
    "\n",
    "First-class passengers had easier access to lifeboats, wider staircases, and closer proximity to the deck. Second-class passengers had fewer advantages, and third-class passengers faced long corridors and locked gates.  \n",
    "\n",
    "In the chaos of the sinking, survival was not just about courage, it was shaped by **where you were in the ship’s hierarchy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_class = bar_chart(\"Pclass\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f4639",
   "metadata": {},
   "source": [
    "## Port of Embarkation and Survival\n",
    "\n",
    "Where passengers boarded the Titanic—**Southampton (S), Cherbourg (C), or Queenstown (Q)**—also influenced their chances of survival.  \n",
    "The port was more than a starting point; it often reflected class, cabin location, and access to lifeboats.  \n",
    "\n",
    "Passengers who boarded at **Cherbourg (C)** were more likely to be first-class and closer to the upper decks, giving them a higher chance of survival. Those from **Southampton (S)** and **Queenstown (Q)** included more second- and third-class passengers, who faced longer routes to safety and crowded corridors.  \n",
    "\n",
    "In the tragedy of that night, survival was shaped not only by courage, but also by **where you entered the ship**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34227b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_embarked = bar_chart(\"Embarked\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164913dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = datasets['train'][\"Survived\"].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "label_percentages = datasets['train'][\"Survived\"].value_counts(normalize=True) * 100\n",
    "print(label_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasets[\"train\"][\"Survived\"].value_counts().values \n",
    "labels = datasets[\"train\"][\"Survived\"].value_counts().index \n",
    "\n",
    "# Use the correct syntax for barplot\n",
    "sns.barplot(x=labels, y=x)\n",
    "plt.title('Frequency Table of the Label')\n",
    "plt.xlabel('Diabetes Binary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print the total number of labels\n",
    "print('Total number of labels: ', sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43332597",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = datasets[\"train\"].select_dtypes(include=[\"number\"]).corr(method=\"spearman\")\n",
    "corr1 = corr.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809242a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "  \n",
    "f, ax = plt.subplots(figsize=(16, 14))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    # xticks\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    # yticks\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    # plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a737efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(corr1, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "  \n",
    "f, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "sns.heatmap(corr1, annot=True, fmt=\".2f\", mask=mask, vmin=0, vmax=1)\n",
    "    # xticks\n",
    "plt.xticks(range(len(corr1.columns)), corr1.columns);\n",
    "    # yticks\n",
    "plt.yticks(range(len(corr1.columns)), corr1.columns)\n",
    "    # plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea393bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = datasets[\"train\"].select_dtypes(include='number').columns\n",
    "fig, axes = plt.subplots(nrows=len(numeric_cols), ncols=1, figsize=(10, 5*len(numeric_cols)))\n",
    "\n",
    "for ax, col in zip(axes, numeric_cols):\n",
    "    ax.hist(datasets[\"train\"][col], bins=50, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5108c06",
   "metadata": {},
   "source": [
    "## **4. Feature Engineering**\n",
    "\n",
    "Feature engineering is the process of **transforming raw data into meaningful features** that improve model performance.  \n",
    "It involves creating new variables, encoding categorical data, handling missing values, and selecting the most informative attributes.  \n",
    "\n",
    "Good feature engineering leverages insights from Exploratory Data Analysis (EDA) to **highlight patterns, enhance predictive power, and make the data more suitable for modeling**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7a16a",
   "metadata": {},
   "source": [
    "### 5.1. Title of Each Passenger\n",
    "\n",
    "The **title** of a passenger, contained in each passengers name, such as *Mr*, *Mrs*, *Miss*, or *Master* provides valuable information about their **social status, age group, and gender**.  \n",
    "\n",
    "Titles can help us understand survival patterns on the Titanic, as certain groups (like women and children) were more likely to survive.  \n",
    "Rare or unusual titles are grouped into a **\"Rare\"** category to simplify the analysis, and missing titles are labeled as **\"Unknown\"**.  \n",
    "\n",
    "By converting titles into **numeric categories**, we create a feature that can improve predictive modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_titles = ['Dr', 'Rev', 'Col', 'Major', 'Mlle', 'Countess', 'Ms', 'Lady', \n",
    "               'Jonkheer', 'Don', 'Capt', 'Sir']\n",
    "title_mapping = {\"Mr\":0, \"Miss\":1, \"Mrs\":2, \"Master\":3, \"Rare\":4, \"Unknown\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['train', 'test']:\n",
    "    datasets[key] = process_titles(datasets[key], rare_titles, title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382af9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_title = bar_chart(\"Title\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7583b53",
   "metadata": {},
   "source": [
    "### 5.2. Family Size\n",
    "\n",
    "The **family size** of a each passenger is calculated as the sum of siblings/spouses (`SibSp`) and parents/children (`Parch`) aboard, plus one for the passenger themselves can provide insights into survival patterns.  \n",
    "\n",
    "Passengers traveling alone often had different survival chances compared to those in larger families. To capture this, we create additional features:  \n",
    "\n",
    "- **FamilySize**: total number of family members aboard  \n",
    "- **FamilyCategory**: a categorical representation of family size (e.g., Single, Small, Large)  \n",
    "- **IsAlone**: a binary indicator of whether the passenger was traveling alone  \n",
    "\n",
    "These features help models understand **social dynamics and group behavior**, which were crucial factors during the Titanic disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f88d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_mapping = {'Single': 0, 'SmallFamily': 1, 'LargeFamily': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\"train\", \"test\"]:\n",
    "    datasets[key] = process_family_features(datasets[key], family_category, family_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310348db",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786045f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7003a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"FamilySize\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_familysize = bar_chart(\"FamilySize\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_familycategoty = bar_chart(\"FamilyCategory\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_isalone = bar_chart(\"IsAlone\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76327c6e",
   "metadata": {},
   "source": [
    "### 5.3) Age Category\n",
    "\n",
    "A passenger's **age** played a role in survival on the Titanic, as children and younger passengers were often prioritized during evacuation.  \n",
    "\n",
    "To capture this, we create the following features:  \n",
    "\n",
    "- **AgeMissing**: a flag indicating if the age was missing, which can itself be informative  \n",
    "- **Age**: missing ages are filled using the median age of passengers with the same **Title**, preserving social/age patterns  \n",
    "- **AgeGroup**: passengers are categorized into **Child, Teen, Adult, MiddleAge, and Senior**, and these groups are mapped to numeric codes for modeling  \n",
    "\n",
    "These age-related features help the model understand patterns related to **age and survival**, while handling missing or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90531dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,12,18,35,60,120]\n",
    "labels = [\"Child\",\"Teen\",\"Adult\",\"MiddleAge\",\"Senior\"]\n",
    "age_mapping = {\"Child\": 0,\"Teen\": 1,\"Adult\": 2,\"MiddleAge\": 3,\"Senior\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a404907",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\"train\",\"test\"]:\n",
    "    datasets[key] = process_age_features(datasets[key],bins,labels,age_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_age = bar_chart(\"AgeGroup\",\"train\",datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252c12f",
   "metadata": {},
   "source": [
    "### 5.4. Fare Prices Category\n",
    "\n",
    "Ticket fares on the Titanic varied greatly, from a few pounds to extravagant sums.  \n",
    "Because the distribution of fares is highly **skewed**, we convert the continuous `Fare` values into **four quantile-based categories** (quartiles).  \n",
    "\n",
    "- **FareBin**: divides passengers into 4 groups (0 = lowest fares, 3 = highest fares)  \n",
    "- This reduces the effect of extreme outliers and allows the model to capture **relative wealth levels** more effectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\"train\", \"test\"]:\n",
    "    datasets[key] = process_fare_bins(datasets[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b571fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbda41",
   "metadata": {},
   "source": [
    "### 5.5) Gender\n",
    "\n",
    "Following the principle of *\"women and children first\"*, women were far more likely to be given places in lifeboats.  \n",
    "\n",
    "To capture this, we encode gender into a numeric feature:  \n",
    "\n",
    "- **SexLabels**:  \n",
    "  - 0 = Male  \n",
    "  - 1 = Female  \n",
    "\n",
    "This transformation allows models to directly use gender as a feature while preserving the critical survival pattern linked to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_mapping = {\"male\": 0, \"female\": 1}\n",
    "for df in [datasets[\"train\"],datasets[\"test\"]]:\n",
    "    df[\"SexLabels\"] = df[\"Sex\"].map(sex_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb1ccf",
   "metadata": {},
   "source": [
    "### 5.6) Embarked One-Hot Encoding\n",
    "\n",
    "The port of embarkation where passengers boarded the Titanic can provide insight into survival patterns.  \n",
    "\n",
    "To make this feature usable for machine learning models, we apply **one-hot encoding**:  \n",
    "\n",
    "- Each port (`C`, `Q`, `S`) is converted into a separate binary column:  \n",
    "  - `Embarked_C` = 1 if the passenger embarked at Cherbourg, else 0  \n",
    "  - `Embarked_Q` = 1 if the passenger embarked at Queenstown, else 0  \n",
    "  - `Embarked_S` = 1 if the passenger embarked at Southampton, else 0  \n",
    "- **Missing values** are captured in an additional column (`Embarked_nan`) to preserve all information.  \n",
    "\n",
    "This transformation allows models to directly use the port of embarkation while handling categorical values and missing data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\"train\", \"test\"]:\n",
    "    datasets[key] = encode_categorical(datasets[key], \"Embarked\", prefix=\"Embarked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5288282",
   "metadata": {},
   "source": [
    "### 5.7) Cabin One-Hot Encoding\n",
    "\n",
    "Cabins on the Titanic were labeled with letters indicating the **deck level**.  \n",
    "Passengers’ location on the ship affected their **access to lifeboats** and thus survival chances.  \n",
    "\n",
    "We extract the **first letter** of the Cabin as the deck and apply **one-hot encoding**:  \n",
    "\n",
    "- Each deck (`A`–`G`) and missing cabins are converted into separate binary columns (`Deck_A`, `Deck_B`, …, `Deck_Missing`).  \n",
    "- This ensures that models can use **deck information numerically** while handling missing values consistently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decks = ['Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_F','Deck_G','Deck_Missing','Deck_T']\n",
    "\n",
    "for key in [\"train\", \"test\"]:\n",
    "    datasets[key] = process_deck(datasets[key], all_decks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bbe21",
   "metadata": {},
   "source": [
    "### 5.8) Ticket\n",
    "\n",
    "The `Ticket` column contains alphanumeric ticket numbers.  \n",
    "While the raw ticket string is not directly useful for modeling, we can extract useful features from it:  \n",
    "\n",
    "- **TicketGroupSize**: counts how many passengers share the same ticket, capturing families or travel companions.  \n",
    "- **TicketPrefix**: extracts any letter or symbol prefix, which may reflect booking type or passenger group.  \n",
    "\n",
    "After extracting these features, the original `Ticket` column is dropped to keep the dataset clean for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to datasets\n",
    "# Compute ticket counts from training set to ensure consistency\n",
    "datasets[\"train\"], ticket_counts = process_ticket(datasets[\"train\"])\n",
    "\n",
    "# Apply same counts to test set\n",
    "datasets[\"test\"], _ = process_ticket(datasets[\"test\"], ticket_counts=ticket_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c413b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing TicketGroupSize in test set\n",
    "datasets[\"test\"]['TicketGroupSize'] = datasets[\"test\"]['TicketGroupSize'].fillna(1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"TicketPrefix\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f923b",
   "metadata": {},
   "source": [
    "The `TicketPrefix` column is messy because it contains many unique values, and most of them appear only once.  \n",
    "Using it directly in models is difficult, especially with one-hot encoding, because it would create hundreds of mostly empty columns that don’t help the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequent prefixes (example: those appearing at least 10 times)\n",
    "freq_prefixes = datasets[\"train\"]['TicketPrefix'].value_counts()[lambda x: x >= 10].index.tolist()\n",
    "\n",
    "# Map rare prefixes to \"Rare\"\n",
    "for df in [datasets[\"train\"], datasets[\"test\"]]:\n",
    "    df['TicketPrefix'] = df['TicketPrefix'].apply(lambda x: x if x in freq_prefixes else 'Rare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"TicketPrefix\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_prefix_dummies = pd.get_dummies(datasets[\"train\"]['TicketPrefix'], prefix='TicketPrefix').astype(int)\n",
    "datasets[\"train\"] = pd.concat([datasets[\"train\"], ticket_prefix_dummies], axis=1)\n",
    "\n",
    "ticket_prefix_dummies_test = pd.get_dummies(datasets[\"test\"]['TicketPrefix'], prefix='TicketPrefix').astype(int)\n",
    "datasets[\"test\"] = pd.concat([datasets[\"test\"], ticket_prefix_dummies_test], axis=1)\n",
    "\n",
    "# Ensure same columns in train and test\n",
    "for col in ticket_prefix_dummies.columns:\n",
    "    if col not in datasets[\"test\"]:\n",
    "        datasets[\"test\"][col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].drop(columns=[\"TicketPrefix\"], inplace=True)\n",
    "datasets[\"test\"].drop(columns=[\"TicketPrefix\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58724f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef21d2",
   "metadata": {},
   "source": [
    "### 5.9. Pclass x Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de714216",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deck in ['Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_F','Deck_G','Deck_Missing','Deck_T']:\n",
    "    for df in [datasets[\"train\"], datasets[\"test\"]]:\n",
    "        df[f'{deck}_Pclass'] = df[deck] * df['Pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49bc71",
   "metadata": {},
   "source": [
    "### 5.10. Sex x AgeGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [datasets[\"train\"], datasets[\"test\"]]:\n",
    "    df['Sex_AgeGroup'] = df['SexLabels'] * df['AgeGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35192ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [datasets[\"train\"], datasets[\"test\"]]:\n",
    "    df['FamilySize_Pclass'] = df['FamilySize'] * df['Pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3702e",
   "metadata": {},
   "source": [
    "## 6) Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002a8f9",
   "metadata": {},
   "source": [
    "### Removing Unnecessary Original Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"PassengerId\",\"Name\",\"Sex\",\"Fare\",\"Deck\",\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = datasets[\"train\"].drop(columns=columns_to_drop, axis = 1)\n",
    "test_clean = datasets[\"test\"].drop(columns=columns_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_clean.drop(['Survived'], axis=1) \n",
    "y_train_true = train_clean['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb294f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only on training data, then transform both\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4953c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns match training set\n",
    "X_test_scaled = X_test_scaled.reindex(columns=X_train_scaled.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ccf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "cross_validation = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "error_metrics = ['accuracy', 'roc_auc', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f79f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('MLP', MLPClassifier()),\n",
    "    ('RFC', RandomForestClassifier()),\n",
    "    ('SVC', SVC()),\n",
    "    ('AdaB', AdaBoostClassifier()),\n",
    "    ('GBC', GradientBoostingClassifier()),\n",
    "    ('DTC', DecisionTreeClassifier()),\n",
    "    ('XGB', XGBClassifier()),\n",
    "    ('LR', LogisticRegression(max_iter=500)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb12cd",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Cross-validation setup\n",
    "num_folds = 5\n",
    "cv = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "error_metrics = ['accuracy', 'f1']  # removed roc_auc\n",
    "\n",
    "trained_models = {}\n",
    "cv_results_summary = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Training model: {name}...\")\n",
    "    \n",
    "    # Fit model on the entire training set\n",
    "    model.fit(X_train_scaled, y_train_true)\n",
    "    \n",
    "    # Store trained model\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    metric_scores = {}\n",
    "    for scoring in error_metrics:\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train_true, cv=cv, scoring=scoring)\n",
    "        metric_scores[scoring] = (scores.mean(), scores.std())\n",
    "        print(f\"{name} - {scoring}: Mean={scores.mean():.4f}, Std={scores.std():.4f}\")\n",
    "    \n",
    "    cv_results_summary.append((name, metric_scores))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ce918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all trained models\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Evaluating model: {name}\")\n",
    "    \n",
    "    # Generate cross-validated predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train_scaled, y_train_true, cv=cv)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_train_true, y_train_pred)\n",
    "    precision = precision_score(y_train_true, y_train_pred)\n",
    "    recall = recall_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_train_true, y_train_pred, name)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c150efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "metrics_summary = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': []\n",
    "}\n",
    "\n",
    "# Loop through all trained models\n",
    "for name, model in models:\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    # Get cross-validated predictions\n",
    "    y_pred = cross_val_predict(model, X_train_scaled, y_train_true, cv=5)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_train_true, y_pred)\n",
    "    prec = precision_score(y_train_true, y_pred)\n",
    "    rec = recall_score(y_train_true, y_pred)\n",
    "    f1 = f1_score(y_train_true, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    metrics_summary['Model'].append(name)\n",
    "    metrics_summary['Accuracy'].append(acc)\n",
    "    metrics_summary['Precision'].append(prec)\n",
    "    metrics_summary['Recall'].append(rec)\n",
    "    metrics_summary['F1'].append(f1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(metrics_df)\n",
    "\n",
    "# Plot bar charts for each metric\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot, 1):\n",
    "    plt.subplot(1, 4, i)\n",
    "    sns.barplot(x='Model', y=metric, data=metrics_df, palette='viridis')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd28b2",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trained_models['XGB'], X_train_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trained_models['RFC'], X_train_scaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553a157",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and coarse grids\n",
    "models_params = {\n",
    "    \"MLP\": {\n",
    "        \"model\": MLPClassifier(max_iter=500, random_state=42),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(50,), (100,)],\n",
    "            \"alpha\": [0.0001, 0.001],\n",
    "            \"learning_rate_init\": [0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    \"RFC\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [None, 5, 10],\n",
    "            \"min_samples_split\": [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"model\": SVC(random_state=42),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"rbf\", \"linear\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    },\n",
    "    \"AdaB\": {\n",
    "        \"model\": AdaBoostClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.5, 1, 1.5]\n",
    "        }\n",
    "    },\n",
    "    \"GBC\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"DTC\": {\n",
    "        \"model\": DecisionTreeClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"max_depth\": [None, 5, 10],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"LR\": {\n",
    "        \"model\": LogisticRegression(max_iter=500, random_state=42),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabadd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, mp in models_params.items():\n",
    "    print(f\"\\nRunning GridSearch for {name}...\")\n",
    "    grid = GridSearchCV(mp[\"model\"], mp[\"params\"], cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train_true)\n",
    "    print(f\"Best F1: {grid.best_score_:.4f} | Best Params: {grid.best_params_}\")\n",
    "    best_models[name] = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8e9c7",
   "metadata": {},
   "source": [
    "## 7) Making Predictions on TEST SET - USING THE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained XGBoost model from grid search\n",
    "xgb_model = best_models['XGB']  # your tuned XGB from GridSearch\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Probabilities (for ROC curves or metrics)\n",
    "y_test_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': datasets['test']['PassengerId'],\n",
    "    'Survived': y_test_pred\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('titanic_xgb_predictions.csv', index=False)\n",
    "\n",
    "print(\"Submission file saved: titanic_xgb_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d916c",
   "metadata": {},
   "source": [
    "## 🏁 Conclusion\n",
    "\n",
    "- The MLP and XGB model achieved the highest accuracy of 82%.\n",
    "- Newly engineered feature Title was the most important predictors of survival.\n",
    "- Future improvements could include hyperparameter tuning and ensemble stacking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
